---
sidebar_position: 5
---

# Module 4: Vision-Language-Action (VLA)

## Focus
**The convergence of LLMs and Robotics.**

The cutting edge of Physical AI is the Vision-Language-Action (VLA) model, where robots understand natural language and visual inputs to perform complex actions.

## Key Topics

### Voice-to-Action
-   Using **OpenAI Whisper** for high-accuracy voice commands.
-   Mapping voice intent to robot primitives ("Walk forward", "Stop").

### Cognitive Planning
-   Using LLMs (like GPT-4) to translate high-level natural language ("Clean the room") into a sequence of ROS 2 actions.
-   Prompt engineering for robotic control.

### Capstone Project: The Autonomous Humanoid
A final project where a simulated robot:
1.  Receives a voice command.
2.  Plans a path.
3.  Navigates obstacles.
4.  Identifies an object using computer vision.
5.  Manipulates it.

## Weekly Breakdown

### Week 13: Conversational Robotics
-   Integrating GPT models for conversational AI in robots
-   Speech recognition and natural language understanding
-   Multi-modal interaction: speech, gesture, vision

## Assessment
**Capstone**: Simulated humanoid robot with conversational AI.
